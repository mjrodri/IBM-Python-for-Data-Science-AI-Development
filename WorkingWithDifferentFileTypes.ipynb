{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import piplite\nawait piplite.install(['seaborn', 'lxml', 'openpyxl'])\n\nimport pandas as pd\n\nfrom pyodide.http import pyfetch\n\nfilename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/addresses.csv\"\n\nasync def download(url, filename):\n    response = await pyfetch(url)\n    if response.status == 200:\n        with open(filename, \"wb\") as f:\n            f.write(await response.bytes())\n\nawait download(filename, \"addresses.csv\")\n\ndf = pd.read_csv(\"addresses.csv\", header=None)\n\ndf\n\ndf.columns =['First Name', 'Last Name', 'Location ', 'City','State','Area Code']\n\ndf\n\ndf[\"First Name\"]\n\ndf = df[['First Name', 'Last Name', 'Location ', 'City','State','Area Code']]\ndf\n\n# To select the first row\ndf.loc[0]\n\n# To select the 0th,1st and 2nd row of \"First Name\" column only\ndf.loc[[0,1,2], \"First Name\" ]\n\n# To select the 0th,1st and 2nd row of \"First Name\" column only\ndf.iloc[[0,1,2], 0]\n\n#import library\nimport pandas as pd\nimport numpy as np\n\n#creating a dataframe\ndf=pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['a', 'b', 'c'])\ndf\n\n#applying the transform function\ndf = df.transform(func = lambda x : x + 10)\ndf\n\nresult = df.transform(func = ['sqrt'])\nresult\n\nimport json\n\nimport json\nperson = {\n    'first_name' : 'Mark',\n    'last_name' : 'abc',\n    'age' : 27,\n    'address': {\n        \"streetAddress\": \"21 2nd Street\",\n        \"city\": \"New York\",\n        \"state\": \"NY\",\n        \"postalCode\": \"10021-3100\"\n    }\n}\n\nwith open('person.json', 'w') as f:  # writing JSON object\n    json.dump(person, f)\n    \n# Serializing json  \njson_object = json.dumps(person, indent = 4) \n  \n# Writing to sample.json \nwith open(\"sample.json\", \"w\") as outfile: \n    outfile.write(json_object) \n    \nprint(json_object)\n\nimport json \n  \n# Opening JSON file \nwith open('sample.json', 'r') as openfile: \n  \n    # Reading from json file \n    json_object = json.load(openfile) \n  \nprint(json_object) \nprint(type(json_object)) \n\nimport pandas as pd\n\n# Not needed unless you're running locally\n# import urllib.request\n# urllib.request.urlretrieve(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/file_example_XLSX_10.xlsx\", \"sample.xlsx\")\n\nfilename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/file_example_XLSX_10.xlsx\"\n\nasync def download(url, filename):\n    response = await pyfetch(url)\n    if response.status == 200:\n        with open(filename, \"wb\") as f:\n            f.write(await response.bytes())\n\nawait download(filename, \"file_example_XLSX_10.xlsx\")\n\ndf = pd.read_excel(\"file_example_XLSX_10.xlsx\")\n\ndf\n\nimport xml.etree.ElementTree as ET\n\n# create the file structure\nemployee = ET.Element('employee')\ndetails = ET.SubElement(employee, 'details')\nfirst = ET.SubElement(details, 'firstname')\nsecond = ET.SubElement(details, 'lastname')\nthird = ET.SubElement(details, 'age')\nfirst.text = 'Shiv'\nsecond.text = 'Mishra'\nthird.text = '23'\n\n# create a new XML file with the results\nmydata1 = ET.ElementTree(employee)\n# myfile = open(\"items2.xml\", \"wb\")\n# myfile.write(mydata)\nwith open(\"new_sample.xml\", \"wb\") as files:\n    mydata1.write(files)\n    \n# Not needed unless running locally\n# !wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/Sample-employee-XML-file.xml\n\nimport xml.etree.ElementTree as etree\n\nfilename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/Sample-employee-XML-file.xml\"\n\nasync def download(url, filename):\n    response = await pyfetch(url)\n    if response.status == 200:\n        with open(filename, \"wb\") as f:\n            f.write(await response.bytes())\n\nawait download(filename, \"Sample-employee-XML-file.xml\")\n\ntree = etree.parse(\"Sample-employee-XML-file.xml\")\n\nroot = tree.getroot()\ncolumns = [\"firstname\", \"lastname\", \"title\", \"division\", \"building\",\"room\"]\n\ndatatframe = pd.DataFrame(columns = columns)\n\nfor node in root: \n\n    firstname = node.find(\"firstname\").text\n\n    lastname = node.find(\"lastname\").text \n\n    title = node.find(\"title\").text \n    \n    division = node.find(\"division\").text \n    \n    building = node.find(\"building\").text\n    \n    room = node.find(\"room\").text\n    \n    datatframe = datatframe.append(pd.Series([firstname, lastname, title, division, building, room], index = columns), ignore_index = True)\n\ndatatframe\n    \n# Herein xpath we mention the set of xml nodes to be considered for migrating  to the dataframe which in this case is details node under employees.\ndf=pd.read_xml(\"Sample-employee-XML-file.xml\", xpath=\"/employees/details\") \n\ndatatframe.to_csv(\"employee.csv\", index=False)\n    \n# importing PIL \nfrom PIL import Image \n\n# Uncomment if running locally\n# import urllib.request\n# urllib.request.urlretrieve(\"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/dog-puppy-on-garden-royalty-free-image-1586966191.jpg\", \"dog.jpg\")\n\nfilename = \"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/dog-puppy-on-garden-royalty-free-image-1586966191.jpg\"\n\nasync def download(url, filename):\n    response = await pyfetch(url)\n    if response.status == 200:\n        with open(filename, \"wb\") as f:\n            f.write(await response.bytes())\n\nawait download(filename, \"dog.jpg\")\n\n# Read image \nimg = Image.open('dog.jpg') \n  \n# Output Images \ndisplay(img)\n\n# Import pandas library\nimport pandas as pd\n\nfilename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/diabetes.csv\"\n\nasync def download(url, filename):\n    response = await pyfetch(url)\n    if response.status == 200:\n        with open(filename, \"wb\") as f:\n            f.write(await response.bytes())\n\nawait download(filename, \"diabetes.csv\")\ndf = pd.read_csv(\"diabetes.csv\")\n\n# show the first 5 rows using dataframe.head() method\nprint(\"The first 5 rows of the dataframe\") \ndf.head(5)\n\ndf.shape\n\ndf.info()\n\ndf.describe()\n\nmissing_data = df.isnull()\nmissing_data.head(5)\n\n<h4>Count missing values in each column</h4>\n<p>\nUsing a for loop in Python, we can quickly figure out the number of missing values in each column. As mentioned above, \"True\" represents a missing value, \"False\"  means the value is present in the dataset.  In the body of the for loop the method \".value_counts()\"  counts the number of \"True\" values. \n</p>\n\nfor column in missing_data.columns.values.tolist():\n    print(column)\n    print (missing_data[column].value_counts())\n    print(\"\")  \n    \n# import libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nlabels= 'Diabetic','Not Diabetic'\nplt.pie(df['Outcome'].value_counts(),labels=labels,autopct='%0.02f%%')\nplt.legend()\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}